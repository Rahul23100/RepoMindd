{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyeFVb6sUHrs+rhnD0pLqM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahul23100/RepoMindd/blob/main/RePoMind_ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FctF9bpJauAM",
        "outputId": "2472bf3e-e8fc-4370-9d77-cf805e225c40"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import re\n",
        "import git\n",
        "import json\n",
        "import time\n",
        "import base64\n",
        "import requests # Used for Ollama API calls\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from datetime import datetime, timedelta\n",
        "from collections import Counter, defaultdict\n",
        "from git.exc import GitCommandError\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "# ===============================================\n",
        "# PAGE CONFIGURATION\n",
        "# ===============================================\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"RepoMind AI - Advanced Repository Analyzer\",\n",
        "    page_icon=\"🧠\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\",\n",
        "    menu_items={\n",
        "        'Get Help': 'https://github.com/yourusername/repomind',\n",
        "        'Report a bug': \"https://github.com/yourusername/repomind/issues\",\n",
        "        'About': \"# RepoMind AI\\nAdvanced AI-powered repository analyzer for understanding GitHub projects instantly.\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# ===============================================\n",
        "# CUSTOM CSS STYLING\n",
        "# ===============================================\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');\n",
        "    .stApp { font-family: 'Inter', sans-serif; }\n",
        "    .main-header {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        -webkit-background-clip: text;\n",
        "        -webkit-text-fill-color: transparent;\n",
        "        background-clip: text;\n",
        "        font-size: 3.5em;\n",
        "        font-weight: 800;\n",
        "        text-align: center;\n",
        "        margin-bottom: 10px;\n",
        "    }\n",
        "    .stButton > button {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        color: white; border: none; padding: 12px 30px; border-radius: 10px; font-weight: 600;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ===============================================\n",
        "# UTILITY AND ANALYSIS FUNCTIONS\n",
        "# ===============================================\n",
        "\n",
        "def get_file_list(repo_path: str) -> List[str]:\n",
        "    \"\"\"Get a list of all file paths in the repository for chatbot context.\"\"\"\n",
        "    file_list = []\n",
        "    for root, dirs, files in os.walk(repo_path):\n",
        "        # Skip hidden directories\n",
        "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
        "        for file in files:\n",
        "            if not file.startswith('.'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                file_list.append(os.path.relpath(full_path, repo_path))\n",
        "    return file_list\n",
        "\n",
        "def get_file_extension_stats(repo_path: str) -> Dict[str, int]:\n",
        "    extensions = defaultdict(int)\n",
        "    for root, dirs, files in os.walk(repo_path):\n",
        "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
        "        for file in files:\n",
        "            if not file.startswith('.'):\n",
        "                ext = Path(file).suffix.lower()\n",
        "                if ext: extensions[ext] += 1\n",
        "    return dict(extensions)\n",
        "\n",
        "def detect_programming_languages(extensions: Dict[str, int]) -> Dict[str, int]:\n",
        "    language_map = {'.py': 'Python', '.js': 'JavaScript', '.ts': 'TypeScript', '.java': 'Java', '.cpp': 'C++', '.c': 'C', '.cs': 'C#', '.rb': 'Ruby', '.go': 'Go', '.rs': 'Rust', '.swift': 'Swift', '.kt': 'Kotlin', '.php': 'PHP', '.r': 'R', '.m': 'MATLAB', '.scala': 'Scala', '.sh': 'Shell', '.html': 'HTML', '.css': 'CSS', '.vue': 'Vue', '.jsx': 'React', '.tsx': 'TypeScript React'}\n",
        "    languages = defaultdict(int)\n",
        "    for ext, count in extensions.items():\n",
        "        if ext in language_map: languages[language_map[ext]] += count\n",
        "    return dict(languages)\n",
        "\n",
        "def analyze_commit_patterns(repo) -> Dict:\n",
        "    commits = list(repo.iter_commits(max_count=500))\n",
        "    commit_hours, commit_days, contributors = Counter(), Counter(), Counter()\n",
        "    commit_months = defaultdict(int)\n",
        "    for commit in commits:\n",
        "        dt = datetime.fromtimestamp(commit.committed_date)\n",
        "        commit_hours[dt.hour] += 1\n",
        "        commit_days[dt.strftime('%A')] += 1\n",
        "        commit_months[dt.strftime('%Y-%m')] += 1\n",
        "        contributors[commit.author.name if commit.author else \"Unknown\"] += 1\n",
        "    recent_commits = [c for c in commits if datetime.fromtimestamp(c.committed_date) > datetime.now() - timedelta(days=30)]\n",
        "    velocity = len(recent_commits) / 30 if recent_commits else 0\n",
        "    return {'total_commits': len(commits), 'commit_hours': dict(commit_hours), 'commit_days': dict(commit_days), 'commit_months': dict(sorted(commit_months.items())[-12:]), 'contributors': dict(contributors.most_common(10)), 'velocity': round(velocity, 2), 'most_active_hour': max(commit_hours, key=commit_hours.get) if commit_hours else None, 'most_active_day': max(commit_days, key=commit_days.get) if commit_days else None}\n",
        "\n",
        "def extract_readme_content(repo_path: str) -> Optional[str]:\n",
        "    for readme in ['README.md', 'readme.md']:\n",
        "        readme_path = os.path.join(repo_path, readme)\n",
        "        if os.path.exists(readme_path):\n",
        "            try:\n",
        "                with open(readme_path, 'r', encoding='utf-8') as f: return f.read()[:5000]\n",
        "            except: continue\n",
        "    return None\n",
        "\n",
        "def calculate_code_metrics(repo_path: str) -> Dict:\n",
        "    total_lines, total_files = 0, 0\n",
        "    largest_file = {\"name\": \"\", \"lines\": 0}\n",
        "    code_extensions = {'.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.ts', '.rb', '.php'}\n",
        "    for root, dirs, files in os.walk(repo_path):\n",
        "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
        "        for file in files:\n",
        "            if Path(file).suffix.lower() in code_extensions:\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        lines = len(f.readlines())\n",
        "                        total_lines += lines; total_files += 1\n",
        "                        if lines > largest_file[\"lines\"]: largest_file = {\"name\": os.path.relpath(file_path, repo_path), \"lines\": lines}\n",
        "                except: continue\n",
        "    return {'total_lines': total_lines, 'total_files': total_files, 'avg_lines_per_file': round(total_lines / total_files) if total_files > 0 else 0, 'largest_file': largest_file}\n",
        "\n",
        "@st.cache_data(show_spinner=False, ttl=3600)\n",
        "def clone_and_analyze_repository(repo_url: str, max_commits: int = 100) -> Tuple[Dict, Optional[str]]:\n",
        "    repo_url = repo_url.strip().rstrip('/') + ('' if repo_url.endswith('.git') else '.git')\n",
        "    repo_name = repo_url.split('/')[-1].replace('.git', '')\n",
        "    local_path = os.path.join(\"/tmp\", f\"repomind_{repo_name}\")\n",
        "    try:\n",
        "        if os.path.exists(local_path):\n",
        "             # Simple pull, for a more robust solution, you might handle conflicts\n",
        "            git.Repo(local_path).remotes.origin.pull()\n",
        "        else:\n",
        "            git.Repo.clone_from(repo_url, local_path, depth=max_commits)\n",
        "\n",
        "        repo = git.Repo(local_path)\n",
        "        # We fetch all commits up to the max_commits depth\n",
        "        commits = list(repo.iter_commits(max_count=max_commits))\n",
        "        commit_data = [{'hash': c.hexsha[:7], 'author': c.author.name if c.author else \"Unknown\", 'date': datetime.fromtimestamp(c.committed_date), 'message': c.message.strip()[:100]} for c in commits]\n",
        "        file_stats = get_file_extension_stats(local_path)\n",
        "        repo_size = sum(os.path.getsize(os.path.join(dirpath, f)) for dirpath, _, filenames in os.walk(local_path) for f in filenames) / (1024*1024)\n",
        "\n",
        "        # NEW: Get file list for chatbot context\n",
        "        file_list = get_file_list(local_path)\n",
        "\n",
        "        return {\n",
        "            'repo_name': repo_name,\n",
        "            'repo_url': repo_url.replace('.git', ''),\n",
        "            'commits': [c.message.strip() for c in commits],\n",
        "            'commit_data': commit_data,\n",
        "            'file_stats': file_stats,\n",
        "            'languages': detect_programming_languages(file_stats),\n",
        "            'commit_patterns': analyze_commit_patterns(repo),\n",
        "            'readme': extract_readme_content(local_path),\n",
        "            'code_metrics': calculate_code_metrics(local_path),\n",
        "            'repo_size': round(repo_size, 2),\n",
        "            'total_files': sum(file_stats.values()),\n",
        "            'branch_count': len(repo.branches),\n",
        "            'last_update': commit_data[0]['date'].strftime('%Y-%m-%d') if commit_data else \"Unknown\",\n",
        "            'file_list': file_list # Added for chatbot\n",
        "        }, None\n",
        "    except Exception as e:\n",
        "        return None, f\"❌ Error: {e}. Check if the URL is correct and the repository is public.\"\n",
        "\n",
        "\n",
        "# ===============================================\n",
        "# AI AND VISUALIZATION FUNCTIONS\n",
        "# ===============================================\n",
        "\n",
        "@st.cache_data(ttl=3600)\n",
        "def generate_ai_summary_ollama(analysis: Dict, ollama_endpoint: str, ollama_model: str) -> Tuple[Optional[str], Optional[str]]:\n",
        "    prompt = f\"You are RepoMind AI, an expert software repository analyst. Analyze this GitHub repository comprehensively.\\n\\nREPOSITORY: {analysis['repo_name']}\\nSTATISTICS:\\n- Total Files: {analysis['total_files']}\\n- Repository Size: {analysis['repo_size']} MB\\n- Commits: {len(analysis['commits'])}\\n- Languages: {', '.join(analysis['languages'].keys())}\\n\\nREADME EXCERPT:\\n{analysis['readme'][:1000] if analysis['readme'] else 'N/A'}\\n\\nBased on this, provide:\\n1. **Project Purpose & Overview**\\n2. **Key Features**\\n3. **Technical Architecture**\\n4. **Development Activity**\\n5. **Recommendations**\\n\\nFormat in clear markdown with emojis.\"\n",
        "    try:\n",
        "        response = requests.post(f\"{ollama_endpoint}/api/generate\", json={\"model\": ollama_model, \"prompt\": prompt, \"stream\": False}, timeout=120)\n",
        "        response.raise_for_status()\n",
        "        return response.json().get('response'), None\n",
        "    except Exception as e: return None, f\"❌ AI Generation Error: {e}\"\n",
        "\n",
        "@st.cache_data(ttl=3600)\n",
        "def generate_code_quality_report_ollama(analysis: Dict, ollama_endpoint: str, ollama_model: str) -> Tuple[Optional[str], Optional[str]]:\n",
        "    prompt = f\"As a senior software architect, analyze the code quality for the '{analysis['repo_name']}' repository.\\n\\nMETRICS:\\n- Total Code Lines: {analysis['code_metrics']['total_lines']}\\n- Avg Lines/File: {analysis['code_metrics']['avg_lines_per_file']}\\n- Largest File: {analysis['code_metrics']['largest_file']['name']} ({analysis['code_metrics']['largest_file']['lines']} lines)\\n\\nProvide a brief code quality assessment covering:\\n1. **Code Organization**\\n2. **Commit Quality**\\n3. **Recommendations**\"\n",
        "    try:\n",
        "        response = requests.post(f\"{ollama_endpoint}/api/generate\", json={\"model\": ollama_model, \"prompt\": prompt, \"stream\": False}, timeout=90)\n",
        "        response.raise_for_status()\n",
        "        return response.json().get('response'), None\n",
        "    except Exception as e: return None, f\"❌ Code quality analysis error: {e}\"\n",
        "\n",
        "# NEW: Chatbot AI function\n",
        "def generate_chatbot_response_ollama(question: str, history: List[Dict], analysis: Dict, ollama_endpoint: str, ollama_model: str) -> str:\n",
        "    \"\"\"Generates a response from the chatbot LLM.\"\"\"\n",
        "    context = f\"\"\"\n",
        "    You are a helpful AI assistant specializing in analyzing the GitHub repository: '{analysis['repo_name']}'.\n",
        "    Use the following context to answer the user's question.\n",
        "\n",
        "    **README Summary:**\n",
        "    {analysis.get('readme', 'Not available.')[:2000]}\n",
        "\n",
        "    **File Structure (sample of files):**\n",
        "    {', '.join(analysis.get('file_list', [])[:50])}\n",
        "\n",
        "    **Main Languages:**\n",
        "    {', '.join(analysis.get('languages', {}).keys())}\n",
        "    \"\"\"\n",
        "\n",
        "    # Simple history formatting\n",
        "    history_str = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in history])\n",
        "\n",
        "    prompt = f\"{context}\\n\\n--- Conversation History ---\\n{history_str}\\n\\n--- New Question ---\\nuser: {question}\\nassistant:\"\n",
        "\n",
        "    try:\n",
        "        response = requests.post(f\"{ollama_endpoint}/api/generate\", json={\"model\": ollama_model, \"prompt\": prompt, \"stream\": False}, timeout=120)\n",
        "        response.raise_for_status()\n",
        "        return response.json().get('response', \"Sorry, I couldn't generate a response.\")\n",
        "    except Exception as e:\n",
        "        return f\"Sorry, an error occurred: {e}\"\n",
        "\n",
        "def create_language_chart(languages: Dict) -> go.Figure:\n",
        "    if not languages: return None\n",
        "    fig = go.Figure(data=[go.Pie(labels=list(languages.keys()), values=list(languages.values()), hole=0.3)])\n",
        "    fig.update_layout(title=\"Languages\", showlegend=True, height=400)\n",
        "    return fig\n",
        "\n",
        "def create_commit_timeline(commit_months: Dict) -> go.Figure:\n",
        "    if not commit_months: return None\n",
        "    fig = go.Figure(data=[go.Scatter(x=list(commit_months.keys()), y=list(commit_months.values()), mode='lines+markers')])\n",
        "    fig.update_layout(title=\"Commit Activity Timeline\", height=350)\n",
        "    return fig\n",
        "\n",
        "# ===============================================\n",
        "# MAIN STREAMLIT APPLICATION\n",
        "# ===============================================\n",
        "\n",
        "def main():\n",
        "    display_header()\n",
        "    if 'repo_url' not in st.session_state: st.session_state.repo_url = \"\"\n",
        "    if 'analysis' not in st.session_state: st.session_state.analysis = None\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.markdown(\"## ⚙️ Configuration\")\n",
        "        ollama_endpoint = \"http://localhost:11434\"\n",
        "        ollama_model = st.text_input(\"Ollama Model Name\", value=\"llama3\", help=\"Ensure this model is pulled in the Colab setup.\")\n",
        "        st.info(f\"Ollama Endpoint is fixed to `{ollama_endpoint}` for Colab.\")\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"## 🎛️ Analysis Settings\")\n",
        "        max_commits = st.slider(\"Commits to Analyze\", 50, 500, 100, 50, help=\"Number of recent commits to fetch and analyze.\")\n",
        "        enable_visualizations = st.checkbox(\"Enable Visualizations\", value=True)\n",
        "        enable_code_quality = st.checkbox(\"Code Quality Report\", value=True)\n",
        "\n",
        "    st.markdown(\"## 🔍 Analyze Repository\")\n",
        "    st.text_input(\"GitHub Repository URL\", key=\"url_input_widget\", value=st.session_state.repo_url, on_change=lambda: setattr(st.session_state, 'repo_url', st.session_state.url_input_widget))\n",
        "    analyze_button = st.button(\"🚀 Analyze\", type=\"primary\")\n",
        "\n",
        "    if analyze_button:\n",
        "        if not st.session_state.repo_url: st.warning(\"Please enter a repository URL.\")\n",
        "        else:\n",
        "            with st.spinner(\"Cloning and analyzing repository... This may take a moment.\"):\n",
        "                analysis, error = clone_and_analyze_repository(st.session_state.repo_url, max_commits)\n",
        "            if error: st.error(error); st.session_state.analysis = None\n",
        "            else: st.session_state.analysis = analysis; st.session_state.messages = [] # Reset chat on new analysis\n",
        "\n",
        "    if st.session_state.analysis:\n",
        "        analysis = st.session_state.analysis\n",
        "        st.markdown(f\"## 📊 Analysis for `{analysis['repo_name']}`\")\n",
        "\n",
        "        # UPDATED: Added a new Chatbot tab\n",
        "        tab1, tab2, tab3, tab4 = st.tabs([\"🤖 AI Summary\", \"📈 Code & Commits\", \"💬 Chat with Repo\", \"📖 README\"])\n",
        "\n",
        "        with tab1:\n",
        "            with st.spinner(f\"🧠 Generating AI summary with {ollama_model}...\"):\n",
        "                summary, error = generate_ai_summary_ollama(analysis, ollama_endpoint, ollama_model)\n",
        "                if error: st.error(error)\n",
        "                else: st.markdown(summary)\n",
        "            if enable_code_quality:\n",
        "                with st.spinner(\"🔬 Assessing code quality...\"):\n",
        "                    report, error = generate_code_quality_report_ollama(analysis, ollama_endpoint, ollama_model)\n",
        "                    if error: st.error(error)\n",
        "                    else: st.markdown(\"---\"); st.markdown(report)\n",
        "\n",
        "        with tab2:\n",
        "            st.markdown(f\"### 📜 Full Commit History (Last {len(analysis['commit_data'])} Commits)\")\n",
        "            if analysis['commit_data']:\n",
        "                # UPDATED: Displaying the full commit dataframe, not just top 20\n",
        "                df = pd.DataFrame(analysis['commit_data'])\n",
        "                df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d %H:%M')\n",
        "                st.dataframe(df, use_container_width=True, hide_index=True)\n",
        "            if enable_visualizations:\n",
        "                st.plotly_chart(create_commit_timeline(analysis['commit_patterns']['commit_months']), use_container_width=True)\n",
        "                st.plotly_chart(create_language_chart(analysis['languages']), use_container_width=True)\n",
        "\n",
        "        # NEW: Chatbot tab implementation\n",
        "        with tab3:\n",
        "            st.markdown(\"### 💬 Ask Questions About This Repository\")\n",
        "            if \"messages\" not in st.session_state:\n",
        "                st.session_state.messages = []\n",
        "\n",
        "            for message in st.session_state.messages:\n",
        "                with st.chat_message(message[\"role\"]):\n",
        "                    st.markdown(message[\"content\"])\n",
        "\n",
        "            if prompt := st.chat_input(\"What is the main purpose of this project?\"):\n",
        "                st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "                with st.chat_message(\"user\"):\n",
        "                    st.markdown(prompt)\n",
        "\n",
        "                with st.chat_message(\"assistant\"):\n",
        "                    with st.spinner(\"Thinking...\"):\n",
        "                        response = generate_chatbot_response_ollama(prompt, st.session_state.messages, analysis, ollama_endpoint, ollama_model)\n",
        "                        st.markdown(response)\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        with tab4:\n",
        "            st.markdown(analysis['readme'] or \"No README found.\")\n",
        "\n",
        "def display_header():\n",
        "    st.markdown('<h1 class=\"main-header\">🧠 RepoMind AI on Colab</h1>', unsafe_allow_html=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "hi977mVkls14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f475fc-afae-4687-e8e8-17370d0e700a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok pandas gitpython plotly requests -q"
      ],
      "metadata": {
        "id": "04DH_P5AXrOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d8273d-46c5-448b-ac32-ce3ce6bbe426"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "# Start Ollama server as a background process\n",
        "async def run_ollama_serve():\n",
        "    process = await asyncio.create_subprocess_shell(\n",
        "        \"ollama serve\",\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "    print(\"Ollama server started.\")\n",
        "    # The server will run in the background\n",
        "    # We don't await process.wait() because we want it to run indefinitely\n",
        "\n",
        "# Run the async function\n",
        "try:\n",
        "    loop = asyncio.get_running_loop()\n",
        "except RuntimeError:  # 'RuntimeError: There is no current event loop...'\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "\n",
        "loop.create_task(run_ollama_serve())\n",
        "\n",
        "# Give the server a moment to start up\n",
        "print(\"Waiting for Ollama server to start...\")\n",
        "!sleep 5\n",
        "\n",
        "# Pull the model\n",
        "print(\"Pulling the Llama3 model. This will take a few minutes...\")\n",
        "!ollama pull llama3\n",
        "print(\"Model pull complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-nkaP-RbH_u",
        "outputId": "1151ad06-2e63-458a-f1d3-f90cf5bd6d42"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for Ollama server to start...\n",
            "Pulling the Llama3 model. This will take a few minutes...\n",
            "Error: ollama server not responding - could not connect to ollama server, run 'ollama serve' to start it\n",
            "Model pull complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSE0KXiZdPjw",
        "outputId": "0a9e4ff7-a20e-4829-ae5b-87cd5d1209fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xvoga7ehINZ",
        "outputId": "54042a78-47a2-42af-c32e-1389700bc3fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.50.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok http 8501 --region in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJm7aeEOv0dl",
        "outputId": "e9da2ee3-4a47-46e0-8f4f-08932d17f0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flag --region has been deprecated, ngrok automatically chooses the region with lowest latency\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual ngrok authtoken\n",
        "NGROK_AUTHTOKEN = \"33Yilzhz7P2407iVh5u1WuBHuFx_7yrwB8zBDurH75eqWtkZm\"\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "import os\n",
        "\n",
        "# Set ngrok authtoken\n",
        "if NGROK_AUTHTOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "    print(\"Ngrok authtoken set.\")\n",
        "else:\n",
        "    print(\"WARNING: Ngrok authtoken not set. Public URL will be temporary.\")\n",
        "\n",
        "# Get the public URL from ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"🚀 Your Streamlit app is live at: {public_url}\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "# The '!' runs this as a shell command.\n",
        "# The '&' runs it in the background.\n",
        "!streamlit run app.py &"
      ],
      "metadata": {
        "id": "dBZF3fuybLmo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}